{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Symbolic Learning\n",
    "Ruchi Gupte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray,save,load\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import idx2numpy\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import sklearn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'training_image_path'            : \"Jio_imagepath/train-images.idx3-ubyte\", \n",
    "           'test_image_path'               : \"Jio_imagepath/t10k-images.idx3-ubyte\",\n",
    "           'genral_path_incase_of_folder'  : 'Jio_imagepath/New',\n",
    "           'dataset_format'                : \"ubyte\",\n",
    "           'sampleimages'                  : 1000,\n",
    "           'window_size'                   : 5,\n",
    "           'Stride'                        : 3,\n",
    "           'padding'                       : [0,1,0,1],\n",
    "           'image_size'                    : 28,\n",
    "           'clustering_model'              : 'KMEANS',\n",
    "           'n_clusters'                    : 75,\n",
    "           'bandwidth'                     : 300,\n",
    "           'test_skip'                     : 2,\n",
    "           'Consistency_formula'           : 'NPMI'\n",
    "          }\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "        json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0 : Read Stage\n",
    "Taking input as config.json file, take test images and load them and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT----------------------------------\n",
    "with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "padding=config['padding']\n",
    "no_of_sampleimages=config['sampleimages']\n",
    "type_dataset=config['dataset_format']\n",
    "#INPUT----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type_dataset=='folder':\n",
    "    final_image_list = []\n",
    "    for img in glob.glob(config['genral_path_incase_of_folder']+'/*.jpg*'):\n",
    "        temp_image= cv2.imread(img,0)\n",
    "        final_image_list.append(temp_image)\n",
    "    random.shuffle(final_image_list)\n",
    "    testdata, traindata = sklearn.model_selection.train_test_split(final_image_list, train_size=0.4, test_size=0.6)\n",
    "\n",
    "elif type_dataset=='ubyte':\n",
    "    traindata = idx2numpy.convert_from_file(config['training_image_path']) \n",
    "    testdata = idx2numpy.convert_from_file(config['test_image_path']) \n",
    "\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = traindata[:no_of_sampleimages]\n",
    "testdata = testdata[:no_of_sampleimages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_data=[0]*no_of_sampleimages\n",
    "for k in range(no_of_sampleimages):\n",
    "    final_train_data[k]=np.pad(traindata[k], [(padding[0], padding[1]), (padding[2], padding[3])], mode='constant', constant_values=0)\n",
    "    \n",
    "final_test_data=[0]*no_of_sampleimages\n",
    "for k in range(no_of_sampleimages):\n",
    "    final_test_data[k]=np.pad(traindata[k], [(padding[0], padding[1]), (padding[2], padding[3])], mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT-------------------------------------------\n",
    "save('Array_of_training_images1.npy', final_train_data)\n",
    "save('Array_of_test_images1.npy', final_test_data)\n",
    "#OUTPUT-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 :Low Level Feature extraction\n",
    "Taking input as config.json and training data, convert data into patches and store them and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT----------------------------------\n",
    "with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "data = load('Array_of_training_images1.npy')\n",
    "#INPUT----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "widthi=heighti=config['image_size']\n",
    "no_of_sampleimages=config['sampleimages']\n",
    "patch_size=config['window_size']\n",
    "shift=config['Stride']\n",
    "padding=config['padding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_im=[]\n",
    "for k in range(no_of_sampleimages):\n",
    "    patchl=0\n",
    "    patchh=0\n",
    "    image=data[k]\n",
    "    for i in range(0,heighti+padding[0]+padding[1]-(patch_size-1),shift):\n",
    "        for j in range(0,widthi+padding[2]+padding[3]-(patch_size-1),shift):\n",
    "            BLOCK=image[i:i+patch_size,j:j+patch_size]\n",
    "            X_im.append(BLOCK) \n",
    "            patchl=patchl+1\n",
    "        patchh=patchh+1\n",
    "        \n",
    "patchl=int(patchl/patchh)\n",
    "X_im = np.array(X_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT-------------------------------------------\n",
    "config.update( {'patchl' : patchl, #patch length\n",
    "                'patchh' : patchh} ) #patch height\n",
    "with open('config.json', 'w') as f:\n",
    "        json.dump(config, f)\n",
    "save('patchlist1.npy', X_im)\n",
    "#OUTPUT-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Symbolic ID Creation\n",
    "Taking input as config.json and data of image patches, perform clustering on them and gather data like centroids, distances and assign each patch to a appropriate cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT----------------------------------\n",
    "patchlist = load('patchlist1.npy')\n",
    "with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "#INPUT---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size=config['window_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans Done. Time elapsed: 1.2061927596728006 minutes\n",
      "72.37156558036804\n"
     ]
    }
   ],
   "source": [
    "if config['clustering_model']=='KMEANS':\n",
    "    time_start = time.time()\n",
    "    n_clusters=config['n_clusters']\n",
    "    model = KMeans(n_clusters=n_clusters, n_init=10, max_iter=300, tol=1e-4, verbose=0, random_state=None, copy_x=True, algorithm=\"auto\")\n",
    "    model=model.fit(patchlist.reshape(len(patchlist),-1))\n",
    "    labels=model.labels_\n",
    "    centroids = model.cluster_centers_\n",
    "    print(\"Kmeans Done. Time elapsed: {} minutes\".format((time.time()-time_start)/60))\n",
    "    print(time.time()-time_start)\n",
    "\n",
    "elif config['clustering_model']=='MEANSHIFT':\n",
    "    time_start = time.time()\n",
    "    bandwidth=config['bandwidth']\n",
    "    model = MeanShift(bandwidth=bandwidth, bin_seeding=False, cluster_all=True, min_bin_freq=1, n_jobs=1, seeds= None)\n",
    "    model=model.fit(patchlist.reshape(len(patchlist),-1))\n",
    "    labels = model.labels_\n",
    "    centroids = model.cluster_centers_\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters = len(labels_unique)\n",
    "    config.update( {'n_clusters' : n_clusters} )\n",
    "    print (\"MeanShift Done. Time elapsed: {} minutes\".format((time.time()-time_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'copy_x': True,\n",
       " 'init': 'k-means++',\n",
       " 'max_iter': 300,\n",
       " 'n_clusters': 75,\n",
       " 'n_init': 10,\n",
       " 'n_jobs': 'deprecated',\n",
       " 'precompute_distances': 'deprecated',\n",
       " 'random_state': None,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort(sub_li): \n",
    "    sub_li.sort(key = lambda x: x[1]) \n",
    "    return sub_li "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "clusterdata=[]\n",
    "for clus in range(n_clusters):\n",
    "    for i in patchlist[np.where(labels==clus)]:\n",
    "        k=k+1\n",
    "    temp=[clus+1,k]\n",
    "    clusterdata.append(temp)   \n",
    "    k=0 #comment to check cumulative density \n",
    "clusterdata=Sort(clusterdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(n_clusters, 0, -1):\n",
    "    #print(clusterdata[i-1][0],'   :  ', clusterdata[i-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "clustack=[]\n",
    "for clus in range(n_clusters):\n",
    "    for i in patchlist[np.where(labels==clus)]:\n",
    "        temp.append(i)\n",
    "    clustack.append(np.array(temp))\n",
    "    temp=[]\n",
    "clustack=np.array(clustack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=[]\n",
    "for Clusterno in range(n_clusters):\n",
    "    temp=[]\n",
    "    Cent=centroids[Clusterno].reshape(patch_size,patch_size)\n",
    "    k=0\n",
    "    for m in clustack[Clusterno]:\n",
    "        distance = np.linalg.norm(m-Cent)\n",
    "        distance=[int(k),int(distance)]\n",
    "        temp.append(distance)\n",
    "        k=k+1\n",
    "    dist.append(temp)\n",
    "for clus in range(n_clusters):\n",
    "    dist[clus]=np.array(Sort(dist[clus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rows=10  #how many rows and columns in collage\n",
    "Columns=10\n",
    "\n",
    "#Making Collage\n",
    "cluster_no=4 # cluster number\n",
    "Background_white = [255,255,255]\n",
    "final_collage = Image.new('RGBA', (1000, 1000), (255, 255,255))\n",
    "k=0\n",
    "\n",
    "for i in range(0,Columns*100,100):\n",
    "    for j in range(0,Rows*100,100):\n",
    "        temp_image=cv2.copyMakeBorder(clustack[cluster_no-1][int(dist[cluster_no-1][k][0])],0,1,0,1,cv2.BORDER_CONSTANT,value=Background_white)\n",
    "        temp_image = Image.fromarray(temp_image, 'L')\n",
    "        temp_image = temp_image.resize((100,100), resample=Image.NEAREST)\n",
    "        k=k+1\n",
    "        final_collage.paste(temp_image, (j,i))\n",
    "\n",
    "final_collage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT------------------------------------------------- Remove 2 for orignal\n",
    "save('cluster_density1.npy', clusterdata)\n",
    "save('patch_with_clusters1.npy', clustack)\n",
    "save('distance_info_for_clusters1.npy', dist)\n",
    "pickle.dump(model, open('model1.sav', 'wb'))\n",
    "#OUTPUT-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 : Symbolic ID Assignment\n",
    "Taking input as config.json, model, its parameters, and test data, convert them to a symbolic matrix using the model from the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT----------------------------------------------------\n",
    "with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "model = pickle.load(open('model1.sav', 'rb'))\n",
    "test_data=load('Array_of_test_images1.npy',allow_pickle = True )\n",
    "#INPUT-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=model.labels_\n",
    "centroids = model.cluster_centers_\n",
    "n_clusters = config['n_clusters']\n",
    "no_of_sampleimages=config['sampleimages']\n",
    "widthi=heighti=config['image_size']\n",
    "padding=config['padding']\n",
    "patch_size=config['window_size']\n",
    "shift=config['Stride']\n",
    "patchl=config['patchl']\n",
    "patchh=config['patchh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic List Created. Time elapsed: 53.71212753454844 minutes\n"
     ]
    }
   ],
   "source": [
    "#assigning symbolic ID for all test patches\n",
    "test_symboliclist=[]\n",
    "time_start = time.time()\n",
    "for testno in range(no_of_sampleimages):\n",
    "    X_reconstructed=[]\n",
    "    image=test_data[testno]  #to select an image number\n",
    "    for i in range(0,heighti+padding[0]+padding[1]-(patch_size-1),shift):\n",
    "        for j in range(0,widthi+padding[2]+padding[3]-(patch_size-1),shift):\n",
    "            BLOCK=image[i:i+patch_size,j:j+patch_size]\n",
    "            v=model.predict(BLOCK.reshape(1, -1))\n",
    "            X_reconstructed.append(v[0]+1)\n",
    "    test_symboliclist.append(X_reconstructed)\n",
    "print(\"Symbolic List Created. Time elapsed: {} minutes\".format((time.time()-time_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for testno in range(no_of_sampleimages):\n",
    "    test_symboliclist[testno]=np.array(test_symboliclist[testno])\n",
    "    test_symboliclist[testno]=test_symboliclist[testno].reshape(patchl,patchh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2,  2,  2, 48, 35, 56,  2,  2],\n",
       "       [ 2,  2,  2, 48,  3, 17, 50, 56,  2],\n",
       "       [ 2,  2, 48,  3, 65, 16, 28, 20,  2],\n",
       "       [ 2,  2,  3, 32, 75, 34, 51, 50,  2],\n",
       "       [ 2,  6,  8, 73,  2, 48, 51, 68,  2],\n",
       "       [ 2, 44, 42, 59, 48,  3, 32, 54,  2],\n",
       "       [ 2, 44, 28, 60, 67, 69, 54,  2,  2],\n",
       "       [ 2, 43, 23, 26, 64, 59,  2,  2,  2],\n",
       "       [ 2,  2,  2,  2,  2,  2,  2,  2,  2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_symboliclist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstructed=test_symboliclist[999].reshape(patchl*patchh,)\n",
    "#X_reconstructed=test_images[1].reshape(patchl*patchh,)\n",
    "Background_white = [255,255,255]\n",
    "\n",
    "final_image = Image.new('RGBA', (1000, 1000), (255, 255, 255, 255))\n",
    "\n",
    "k=0\n",
    "for i in range(0,patchh*100,100):\n",
    "    for j in range(0,patchl*100,100):\n",
    "        Centroid=centroids[X_reconstructed[k]-1].reshape(patch_size,patch_size)\n",
    "        temp_image=cv2.copyMakeBorder(Centroid,0,1,0,1,cv2.BORDER_CONSTANT,value=Background_white)\n",
    "        temp_image = Image.fromarray(temp_image)\n",
    "        temp_image = temp_image.resize((100,100), resample=Image.NEAREST)\n",
    "        k=k+1\n",
    "        final_image.paste(temp_image, (j,i))\n",
    "               \n",
    "final_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT-------------------------------------------------Remove 2 for orignal\n",
    "save('testdata_symboliclist1.npy', test_symboliclist)\n",
    "#OUTPUT-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_symboliclist=load('testdata_symboliclist1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Hardcoding directional matrix\n",
    "Since the position of matrix elements remains same across same size images, obtain directional matrix values of each element wrt all other elements in that matrix. Input as config.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT-----------------------------------------\n",
    "with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "no_of_sampleimages=config['sampleimages']\n",
    "test_skip=config[\"test_skip\"]\n",
    "symbolic_list=load('testdata_symboliclist1.npy')\n",
    "patch_size=config['window_size']\n",
    "#INPUT-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchlength=config['patchl']\n",
    "patchheight=config['patchh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos2(x): \n",
    "    ans=math.cos(x)*math.cos(x)\n",
    "    return ans\n",
    "\n",
    "def sin2(x): \n",
    "    ans=math.sin(x)*math.sin(x)\n",
    "    return ans\n",
    "\n",
    "def sliding_window(arr, window_size):\n",
    "    \"\"\" Construct a sliding window view of the array\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    window_size = int(window_size)\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(\"need 2-D input\")\n",
    "    if not (window_size > 0):\n",
    "        raise ValueError(\"need a positive window size\")\n",
    "    shape = (arr.shape[0] - window_size + 1,\n",
    "             arr.shape[1] - window_size + 1,\n",
    "             window_size, window_size)\n",
    "    if shape[0] <= 0:\n",
    "        shape = (1, shape[1], arr.shape[0], shape[3])\n",
    "    if shape[1] <= 0:\n",
    "        shape = (shape[0], 1, shape[2], arr.shape[1])\n",
    "    strides = (arr.shape[1]*arr.itemsize, arr.itemsize,\n",
    "               arr.shape[1]*arr.itemsize, arr.itemsize)\n",
    "    return as_strided(arr, shape=shape, strides=strides)\n",
    "\n",
    "def cell_neighbors(arr, i, j, d):\n",
    "    \"\"\"Return d-th neighbors of cell (i, j)\"\"\"\n",
    "    w = sliding_window(arr, 2*d+1)\n",
    "\n",
    "    ix = np.clip(i - d, 0, w.shape[0]-1)\n",
    "    jx = np.clip(j - d, 0, w.shape[1]-1)\n",
    "\n",
    "    i0 = max(0, i - d - ix)\n",
    "    j0 = max(0, j - d - jx)\n",
    "    i1 = w.shape[2] - max(0, d - i + ix)\n",
    "    j1 = w.shape[3] - max(0, d - j + jx)\n",
    "\n",
    "    return w[ix, jx][i0:i1,j0:j1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2,  2,  2, 48, 35, 56,  2,  2],\n",
       "       [ 2,  2,  2, 48,  3, 17, 50, 56,  2],\n",
       "       [ 2,  2, 48,  3, 65, 16, 28, 20,  2],\n",
       "       [ 2,  2,  3, 32, 75, 34, 51, 50,  2],\n",
       "       [ 2,  6,  8, 73,  2, 48, 51, 68,  2],\n",
       "       [ 2, 44, 42, 59, 48,  3, 32, 54,  2],\n",
       "       [ 2, 44, 28, 60, 67, 69, 54,  2,  2],\n",
       "       [ 2, 43, 23, 26, 64, 59,  2,  2,  2],\n",
       "       [ 2,  2,  2,  2,  2,  2,  2,  2,  2]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbolic_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2, 48, 56,  2],\n",
       "       [ 2, 48, 65, 28,  2],\n",
       "       [ 2,  8,  2, 51,  2],\n",
       "       [ 2, 28, 67, 54,  2],\n",
       "       [ 2,  2,  2,  2,  2]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images=[]\n",
    "image=[]\n",
    "for num in range(no_of_sampleimages):\n",
    "    image=symbolic_list[num]\n",
    "    patchl=0\n",
    "    patchh=0\n",
    "    temp=[]\n",
    "    for i in range(0,patchheight,test_skip):\n",
    "        for j in range(0,patchlength,test_skip):\n",
    "            temp.append(image[i][j])\n",
    "            patchl=patchl+1\n",
    "        patchh=patchh+1\n",
    "    \n",
    "    patchl=int(patchl/patchh)\n",
    "    temp=np.array(temp)\n",
    "    temp=temp.reshape(patchh,patchl)\n",
    "    test_images.append(temp)\n",
    "test_images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstructed=test_images[10].reshape(patchl*patchh,)\n",
    "#X_reconstructed=test_images[1].reshape(patchl*patchh,)\n",
    "Background_white = [255,255,255]\n",
    "\n",
    "final_image = Image.new('RGBA', (1000, 1000), (255, 255, 255, 255))\n",
    "\n",
    "k=0\n",
    "for i in range(0,patchh*100,100):\n",
    "    for j in range(0,patchl*100,100):\n",
    "        Centroid=centroids[X_reconstructed[k]-1].reshape(patch_size,patch_size)\n",
    "        temp_image=cv2.copyMakeBorder(Centroid,0,0,0,0,cv2.BORDER_CONSTANT,value=Background_white)\n",
    "        temp_image = Image.fromarray(temp_image)\n",
    "        temp_image = temp_image.resize((100,100), resample=Image.NEAREST)\n",
    "        k=k+1\n",
    "        final_image.paste(temp_image, (j,i))\n",
    "               \n",
    "final_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayorder=[]\n",
    "k=0\n",
    "for y2 in range(patchh):\n",
    "    for x2 in range(patchl):\n",
    "        arrayorder.append(k)\n",
    "        k=k+1\n",
    "arrayorder=np.array(arrayorder)\n",
    "arrayorder=arrayorder.reshape(patchh,patchl)\n",
    "arrayorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make patchh,patchl 5 if we change the dimension of input matrix\n",
    "weights=[]\n",
    "for y1 in range(patchh):\n",
    "    for x1 in range(patchl):\n",
    "        temp=[]\n",
    "        for y2 in range(patchh):\n",
    "            for x2 in range(patchl):\n",
    "                if y2>y1 and x1==x2:\n",
    "                    x=90 #pure north\n",
    "                elif y2<y1 and x1==x2:\n",
    "                    x=-90 #pure south\n",
    "                elif y2==y1 and x2==x1:\n",
    "                    x=0\n",
    "                elif x1>x2 and y1==y2:\n",
    "                    x=404   #pure west\n",
    "                elif x2>x1 and y1==y2:\n",
    "                    x=303  #pure east\n",
    "                else:\n",
    "                    slope = ((y2-y1)/(x2-x1))\n",
    "                    x=math.atan(slope)\n",
    "                temp.append([int(x1),int(y1),int(x2),int(y2),x])\n",
    "        weights.append(temp)\n",
    "#getting angles for each element[point1,point2==>angle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values=[]\n",
    "l=0\n",
    "for y1 in range(patchh):\n",
    "    for x1 in range(patchl):\n",
    "        k=0\n",
    "        element=[]\n",
    "        for y2 in range(patchh):\n",
    "            for x2 in range(patchl):\n",
    "                angle=weights[l][k][4] #radians\n",
    "                if angle==90:\n",
    "                    ans=[1,0,0,0] #north\n",
    "                elif angle==-90:\n",
    "                    ans=[0,1,0,0] #south\n",
    "                elif angle==404:\n",
    "                    ans=[0,0,1,0] #east\n",
    "                elif angle==303:\n",
    "                    ans=[0,0,0,1] #west\n",
    "                elif x1==x2 and y1==y2:\n",
    "                    ans=[0,0,0,0] #samepoint       \n",
    "                elif x1>x2 and y1>y2:\n",
    "                    ans=[0,sin2(angle),0,cos2(angle)] #southwest\n",
    "                elif x1>x2 and y1<y2:\n",
    "                    ans=[sin2(angle),0,0,cos2(angle)] #northwest\n",
    "                elif x1<x2 and y1>y2:\n",
    "                    ans=[0,sin2(angle),cos2(angle),0] #southeast\n",
    "                elif x1<x2 and y1<y2:\n",
    "                    ans=[sin2(angle),0,cos2(angle),0] #northeast\n",
    "                else:\n",
    "                    ans=4040404 #error\n",
    "                element.append([x1,y1,x2,y2,ans]) #all relations for one point\n",
    "                k=k+1 #increment to next point\n",
    "        #print(4040404 in element[4])\n",
    "        values.append(element)  \n",
    "        l=l+1 #increment to next pivot point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values[5]#x1,y1,x2,y2,directionalmatrix for those 2 positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_value=2\n",
    "final=[]\n",
    "c=0\n",
    "for y1 in range(patchh):\n",
    "    for x1 in range(patchl):\n",
    "        ans=cell_neighbors(arrayorder,y1,x1,neighbor_value)\\\n",
    "        #ans=arrayorder.reshape(81,)\n",
    "        #shape=int(math.sqrt(len(ans)))\n",
    "        standby=[]\n",
    "        for i in range(len(ans)):\n",
    "            standby.append(values[c][ans[i]])\n",
    "        final.append(standby) #final gives only those points within neighbor distance\n",
    "        c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"AllHardcoded_directionalmatrix1.npy\", values)\n",
    "save(\"New_symbolic_list1.npy\", test_images)\n",
    "save(\"Selected_Hardcoded_directionalmatrix1.npy\", final)\n",
    "config.update( {'stage4_patchl' : patchl,\n",
    "                'stage4_patchh' : patchh} )\n",
    "with open('config.json', 'w') as f:\n",
    "        json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5: Patch to Patch Co-Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT-----------------------------------------\n",
    "with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "new_symbolic_list=load('New_symbolic_list1.npy')\n",
    "patchl=config['stage4_patchl']\n",
    "patchh=config['stage4_patchh']\n",
    "no_of_sampleimages=config['sampleimages']\n",
    "values=load('AllHardcoded_directionalmatrix1.npy',allow_pickle = True )\n",
    "final=load('Selected_Hardcoded_directionalmatrix1.npy',allow_pickle = True )\n",
    "#INPUT-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co Occurrence Tables Created. Time elapsed: 3.0758633613586426 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "north_table = []\n",
    "south_table=[]\n",
    "east_table=[]\n",
    "west_table=[]\n",
    "for num in range(1000):\n",
    "    trial=new_symbolic_list[num]\n",
    "    for poi in range(len(final)):\n",
    "        for ele in range(len(final[poi])):\n",
    "            data=final[poi][ele]\n",
    "            a_id=trial[data[1]][data[0]]\n",
    "            b_id=trial[data[3]][data[2]]\n",
    "            dictionary_data_north = {'a_id': a_id, 'b_id' : b_id, 'count_north': data[4][0]}\n",
    "            dictionary_data_south= {'a_id': a_id, 'b_id' : b_id, 'count_south': data[4][1]}\n",
    "            dictionary_data_east= {'a_id': a_id, 'b_id' : b_id, 'count_east': data[4][2]}\n",
    "            dictionary_data_west= {'a_id': a_id, 'b_id' : b_id, 'count_west': data[4][3]}          \n",
    "            north_table.append(dictionary_data_north)\n",
    "            south_table.append(dictionary_data_south)\n",
    "            east_table.append(dictionary_data_east)\n",
    "            west_table.append(dictionary_data_west)\n",
    "                \n",
    "north_table = pd.DataFrame.from_dict(north_table)\n",
    "north_table = north_table[(north_table[['count_north']] != 0).all(axis=1)]\n",
    "north_table = pd.pivot_table(north_table, values='count_north', index=['a_id', 'b_id'], aggfunc=np.sum)\n",
    "north_table = north_table.sort_values(('count_north'), ascending=False)\n",
    "\n",
    "south_table = pd.DataFrame.from_dict(south_table)\n",
    "south_table = south_table[(south_table[['count_south']] != 0).all(axis=1)]\n",
    "south_table = pd.pivot_table(south_table, values='count_south', index=['a_id', 'b_id'],aggfunc=np.sum)\n",
    "south_table = south_table.sort_values(('count_south'), ascending=False)\n",
    "\n",
    "east_table = pd.DataFrame.from_dict(east_table)\n",
    "east_table = east_table[(east_table[['count_east']] != 0).all(axis=1)]\n",
    "east_table = pd.pivot_table(east_table, values='count_east', index=['a_id', 'b_id'], aggfunc=np.sum)\n",
    "east_table = east_table.sort_values(('count_east'), ascending=False)\n",
    "\n",
    "west_table = pd.DataFrame.from_dict(west_table)\n",
    "west_table = west_table[(west_table[['count_west']] != 0).all(axis=1)]\n",
    "west_table = pd.pivot_table(west_table, values='count_west', index=['a_id', 'b_id'], aggfunc=np.sum)\n",
    "west_table = west_table.sort_values(('count_west'), ascending=False)\n",
    "\n",
    "print(\"Co Occurrence Tables Created. Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_table.to_csv('north_table1.csv') #Remove 2 for orignal\n",
    "south_table.to_csv('south_table1.csv') \n",
    "east_table.to_csv('east_table1.csv') \n",
    "west_table.to_csv('west_table1.csv') \n",
    "\n",
    "north_table = pd.read_csv(\"north_table1.csv\") \n",
    "south_table = pd.read_csv(\"south_table1.csv\") \n",
    "east_table = pd.read_csv(\"east_table1.csv\") \n",
    "west_table = pd.read_csv(\"west_table1.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NPMI Formula: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistencies Added. Time elapsed: 53.227421045303345 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "north_table['north_consistency'] = 0 \n",
    "total_count_north = north_table['count_north'].sum()\n",
    "for i, j in north_table.iterrows(): \n",
    "    cooc_probability= north_table.loc[(north_table['a_id'] == j[0]) & (north_table['b_id'] == j[1]),'count_north'].sum() / total_count_north\n",
    "    marginal_a_id_probability= north_table.loc[north_table['a_id'] == j[0], 'count_north'].sum() / total_count_north\n",
    "    marginal_b_id_probability= north_table.loc[north_table['b_id'] == j[1], 'count_north'].sum() /total_count_north\n",
    "    consistency_north=float(np.log2(cooc_probability / (marginal_a_id_probability * marginal_b_id_probability))/np.log2(1 / cooc_probability))\n",
    "    north_table.loc[(north_table['a_id'] == j[0]) & (north_table['b_id'] == j[1]), 'north_consistency'] = consistency_north\n",
    "\n",
    "south_table['south_consistency'] = 0 \n",
    "total_count_south = south_table['count_south'].sum()\n",
    "for i, j in south_table.iterrows(): \n",
    "    cooc_probability= south_table.loc[(south_table['a_id'] == j[0]) & (south_table['b_id'] == j[1]),'count_south'].sum() / total_count_south\n",
    "    marginal_a_id_probability= south_table.loc[south_table['a_id'] == j[0], 'count_south'].sum() / total_count_south\n",
    "    marginal_b_id_probability= south_table.loc[south_table['b_id'] == j[1], 'count_south'].sum() /total_count_south\n",
    "    consistency_south=float(np.log2(cooc_probability / (marginal_a_id_probability * marginal_b_id_probability))/np.log2(1 / cooc_probability))\n",
    "    south_table.loc[(south_table['a_id'] == j[0]) & (south_table['b_id'] == j[1]), 'south_consistency'] = consistency_south\n",
    "    \n",
    "east_table['east_consistency'] = 0 \n",
    "total_count_east = east_table['count_east'].sum()\n",
    "for i, j in east_table.iterrows(): \n",
    "    cooc_probability= east_table.loc[(east_table['a_id'] == j[0]) & (east_table['b_id'] == j[1]),'count_east'].sum() / total_count_east\n",
    "    marginal_a_id_probability= east_table.loc[east_table['a_id'] == j[0], 'count_east'].sum() / total_count_east\n",
    "    marginal_b_id_probability= east_table.loc[east_table['b_id'] == j[1], 'count_east'].sum() /total_count_east\n",
    "    consistency_east=float(np.log2(cooc_probability / (marginal_a_id_probability * marginal_b_id_probability))/np.log2(1 / cooc_probability))\n",
    "    east_table.loc[(east_table['a_id'] == j[0]) & (east_table['b_id'] == j[1]), 'east_consistency'] = consistency_east\n",
    "\n",
    "west_table['west_consistency'] = 0 \n",
    "total_count_west = west_table['count_west'].sum()\n",
    "for i, j in west_table.iterrows(): \n",
    "    cooc_probability= west_table.loc[(west_table['a_id'] == j[0]) & (west_table['b_id'] == j[1]),'count_west'].sum() / total_count_west\n",
    "    marginal_a_id_probability= west_table.loc[west_table['a_id'] == j[0], 'count_west'].sum() / total_count_west\n",
    "    marginal_b_id_probability= west_table.loc[west_table['b_id'] == j[1], 'count_west'].sum() /total_count_west\n",
    "    consistency_west=float(np.log2(cooc_probability / (marginal_a_id_probability * marginal_b_id_probability))/np.log2(1 / cooc_probability))\n",
    "    west_table.loc[(west_table['a_id'] == j[0]) & (west_table['b_id'] == j[1]), 'west_consistency'] = consistency_west\n",
    "\n",
    "print(\"Consistencies Added. Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "north_table['north_consistency'] = 0 \n",
    "for i, j in north_table.iterrows(): \n",
    "    plr=north_table.loc[north_table['a_id'] == j[0], 'count_north'].sum()\n",
    "    pll=north_table.loc[north_table['b_id'] == j[1], 'count_north'].sum()\n",
    "    pt=north_table.loc[(north_table['a_id'] == j[0]) & (north_table['b_id'] == j[1]),'count_north'].sum()\n",
    "    consistency=math.log(pt/(plr*pll))\n",
    "    north_table.loc[(north_table['a_id'] == j[0]) & (north_table['b_id'] == j[1]), 'north_consistency'] = consistency\n",
    "\n",
    "south_table['south_consistency'] = 0 \n",
    "for i, j in south_table.iterrows(): \n",
    "    plr=south_table.loc[south_table['a_id'] == j[0], 'count_south'].sum()\n",
    "    pll=south_table.loc[south_table['b_id'] == j[1], 'count_south'].sum()\n",
    "    pt=south_table.loc[(south_table['a_id'] == j[0]) & (south_table['b_id'] == j[1]),'count_south'].sum()\n",
    "    consistency=math.log(pt/(plr*pll))\n",
    "    south_table.loc[(south_table['a_id'] == j[0]) & (south_table['b_id'] == j[1]), 'south_consistency'] = consistency\n",
    "\n",
    "east_table['east_consistency'] = 0 \n",
    "for i, j in east_table.iterrows(): \n",
    "    plr=east_table.loc[east_table['a_id'] == j[0], 'count_east'].sum()\n",
    "    pll=east_table.loc[east_table['b_id'] == j[1], 'count_east'].sum()\n",
    "    pt=east_table.loc[(east_table['a_id'] == j[0]) & (east_table['b_id'] == j[1]),'count_east'].sum()\n",
    "    consistency=math.log(pt/(plr*pll))\n",
    "    east_table.loc[(east_table['a_id'] == j[0]) & (east_table['b_id'] == j[1]), 'east_consistency'] = consistency\n",
    "\n",
    "west_table['west_consistency'] = 0 \n",
    "for i, j in west_table.iterrows(): \n",
    "    plr=west_table.loc[west_table['a_id'] == j[0], 'count_west'].sum()\n",
    "    pll=west_table.loc[west_table['b_id'] == j[1], 'count_west'].sum()\n",
    "    pt=west_table.loc[(west_table['a_id'] == j[0]) & (west_table['b_id'] == j[1]),'count_west'].sum()\n",
    "    consistency=math.log(pt/(plr*pll))\n",
    "    west_table.loc[(west_table['a_id'] == j[0]) & (west_table['b_id'] == j[1]), 'west_consistency'] = consistency\n",
    "\n",
    "print(\"Consistencies Added. Time elapsed: {} seconds\".format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "east_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show(north_table[\"north_consistency\"].plot.hist(bins=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_table = pd.read_csv(\"north_table1.csv\") \n",
    "south_table = pd.read_csv(\"south_table1.csv\") \n",
    "east_table = pd.read_csv(\"east_table1.csv\") \n",
    "west_table = pd.read_csv(\"west_table1.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT------------------------------------------------------\n",
    "north_table.to_csv('north_table1.csv', index=False) #Remove 2 for orignal\n",
    "south_table.to_csv('south_table1.csv', index=False) \n",
    "east_table.to_csv('east_table1.csv', index=False) \n",
    "west_table.to_csv('west_table1.csv', index=False) \n",
    "#OUTPUT-----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
